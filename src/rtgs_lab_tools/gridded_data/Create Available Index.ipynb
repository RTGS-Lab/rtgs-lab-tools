{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-05 11:59:59.420448\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "PLANET_API_KEY='PLAK93320cfc5cdc4acd81c3df746739143a'\n",
    "\n",
    "orders_url = 'https://api.planet.com/compute/ops/orders/v2'\n",
    "\n",
    "# set up requests to work with api\n",
    "auth = HTTPBasicAuth(PLANET_API_KEY, '')\n",
    "headers = {'content-type': 'application/json'}\n",
    "\n",
    "course_code = \"01A\"\n",
    "year = \"2025\"\n",
    "\n",
    "#Directory paths\n",
    "bDir = 'C:\\\\Users\\\\msmurphy\\\\Documents\\\\WinterTurf Planet\\\\Sensor Courses\\\\'\n",
    "iDir = bDir + 'geojson'\n",
    "oDir = bDir + 'images'\n",
    "\n",
    "print(datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-05 12:00:01.481284\n"
     ]
    }
   ],
   "source": [
    "#some json functions\n",
    "\n",
    "def open_geojson(file_path):\n",
    "    with open(file_path) as f:\n",
    "#         gj = geojson.load(f)\n",
    "        gj = json.load(f)\n",
    "        \n",
    "    return gj\n",
    "\n",
    "def get_geojson_geometry(geojson):\n",
    "    geometry = [i['geometry'] for i in geojson['features']]\n",
    "    \n",
    "    return geometry\n",
    "\n",
    "def plot_wrapper(gdf, ax, color, linewidth=1.5):\n",
    "    '''Convenience function for overlaying spatial data on the same plot'''\n",
    "    gdf.plot(\n",
    "        facecolor=\"none\",\n",
    "        edgecolor=color,\n",
    "        linewidth=linewidth,\n",
    "        ax = ax   \n",
    "    )\n",
    "\n",
    "# define helpful functions for submitting, polling, and downloading an order\n",
    "def place_order(request, auth):\n",
    "    response = requests.post(orders_url, data=json.dumps(request), auth=auth, headers=headers)\n",
    "    print(response)\n",
    "    \n",
    "    if not response.ok:\n",
    "        raise Exception(response.content)\n",
    "\n",
    "    order_id = response.json()['id']\n",
    "    print(order_id)\n",
    "    order_url = orders_url + '/' + order_id\n",
    "    return order_url\n",
    "\n",
    "# print(datetime.now())\n",
    "\n",
    "# Function to retrieve all features\n",
    "def get_all_features(search_params, auth):\n",
    "    features = []\n",
    "    offset = 0\n",
    "    while True:\n",
    "        search_params['offset'] = offset\n",
    "        response = requests.get(orders_url, headers=headers, auth=auth, params=search_params)\n",
    "        response.raise_for_status()  # Raise an error for bad requests\n",
    "        data = response.json()\n",
    "        # print(data)\n",
    "        features.extend(data['features'])\n",
    "        if len(data['features']) < 250:\n",
    "            break\n",
    "        offset += 250\n",
    "    return features\n",
    "\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 560 features\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_class_name = \"Course_\"+course_code+\"_Outline\"\n",
    "\n",
    "start_date = year+'-01-01'\n",
    "end_date = year+'-12-31'\n",
    "\n",
    "geojson_file = iDir+\"\\\\\"+feature_class_name+\".geojson\"\n",
    "\n",
    "# Read the GeoJSON file into a GeoDataFrame for the aoi\n",
    "gdf_aoi = gpd.read_file(geojson_file)\n",
    "\n",
    "#Create the geometry for the Planet query\n",
    "boundary_geojson = open_geojson(geojson_file)\n",
    "\n",
    "bounds = get_geojson_geometry(boundary_geojson)\n",
    "\n",
    "geojson_geometry = bounds[0]\n",
    "\n",
    "#Set up filters\n",
    "geometry_filter = {\n",
    "\"type\": \"GeometryFilter\",\n",
    "\"field_name\": \"geometry\",\n",
    "\"config\": geojson_geometry\n",
    "}\n",
    "\n",
    "instrument_filter = {\n",
    "\"type\":\"StringInFilter\",\n",
    "\"field_name\":\"instrument\",\n",
    "\"config\":[\n",
    "    # \"PSB.SD\"\n",
    "    \"PS2.SD\"\n",
    "    # \"PS2\"\n",
    "]\n",
    "}\n",
    "\n",
    "date_range_filter = {\n",
    "\"type\": \"DateRangeFilter\",\n",
    "\"field_name\": \"acquired\",\n",
    "\"config\": {\n",
    "    \"gte\": start_date+\"T00:00:00.000Z\",\n",
    "    \"lte\": end_date+\"T00:00:00.000Z\"\n",
    "}\n",
    "}\n",
    "\n",
    "# orthoanalytic images that have 4 bands and are corrected for surface reflectance\n",
    "asset_filter = {\n",
    "\"type\": \"AssetFilter\",\n",
    "\"config\": [\"ortho_analytic_4b_sr\"]\n",
    "}\n",
    "\n",
    "# combine filters including clear filters\n",
    "was_combined_filter = {\n",
    "\"type\": \"AndFilter\",\n",
    "\"config\": [geometry_filter, date_range_filter, asset_filter]\n",
    "}\n",
    "\n",
    "item_type = \"PSScene\"\n",
    "\n",
    "# API request object\n",
    "search_request = {\n",
    "\"item_types\": [item_type], \n",
    "# \"limit\":250,\n",
    "# \"offset\": 0,\n",
    "\"filter\": was_combined_filter\n",
    "}\n",
    "\n",
    "# # fire off the POST request\n",
    "search_result = \\\n",
    "requests.post(\n",
    "    'https://api.planet.com/data/v1/quick-search?_sort=acquired asc&_page_size=250',\n",
    "    auth=HTTPBasicAuth(PLANET_API_KEY, ''),\n",
    "    json=search_request)\n",
    "\n",
    "\n",
    "search_result.raise_for_status()\n",
    "payload = search_result.json()\n",
    "\n",
    "# Collect results\n",
    "all_features = []\n",
    "all_ids = []\n",
    "\n",
    "# Add first page features\n",
    "features = payload.get(\"features\", []) or []\n",
    "all_features.extend(features)\n",
    "all_ids.extend([f.get(\"id\") for f in features])\n",
    "\n",
    "# Get the next link (if any)\n",
    "links = payload.get(\"_links\", {}) or {}\n",
    "next_url = links.get(\"_next\")\n",
    "\n",
    "SLEEP_SECONDS = 0.25\n",
    "TIMEOUT = 60.0\n",
    "\n",
    "# --- Follow pagination chain (GET) ---\n",
    "while next_url is not None:\n",
    "    if SLEEP_SECONDS:\n",
    "        time.sleep(SLEEP_SECONDS)\n",
    "\n",
    "    nxt = requests.get(next_url, auth=auth, timeout=TIMEOUT)\n",
    "\n",
    "    # Simple retry on rate limit (HTTP 429)\n",
    "    if nxt.status_code == 429:\n",
    "        time.sleep(2.0)\n",
    "        nxt = requests.get(next_url, auth=auth, timeout=TIMEOUT)\n",
    "\n",
    "    nxt.raise_for_status()\n",
    "    payload = nxt.json()\n",
    "\n",
    "    features = payload.get(\"features\", []) or []\n",
    "    all_features.extend(features)\n",
    "    all_ids.extend([f.get(\"id\") for f in features])\n",
    "\n",
    "    links = payload.get(\"_links\", {}) or {}\n",
    "    next_url = links.get(\"_next\")\n",
    "\n",
    "print(f\"Fetched {len(all_features)} features\")\n",
    "\n",
    "#Create a dataframe out of the json response to the search\n",
    "df_tiles = pd.json_normalize(all_features, max_level=1)\n",
    "\n",
    "# rename properties column, dropping the properties label\n",
    "prop_cols = {col: col.split('.')[1] for col in df_tiles.columns if 'properties' in col}\n",
    "df_tiles.rename(columns=prop_cols, inplace=True)\n",
    "\n",
    "geoms = [Polygon(geo[0]) for geo in df_tiles['geometry.coordinates']]\n",
    "gdf_tiles = gpd.GeoDataFrame(df_tiles, geometry = geoms, crs=4326)\n",
    "\n",
    "gdf_tiles_contain_aoi = gpd.sjoin(gdf_tiles, gdf_aoi,  how='inner', predicate='contains' )\n",
    "\n",
    "gdf_tiles_contain_aoi['date_acquired'] = pd.to_datetime(gdf_tiles_contain_aoi['acquired'].str[:10])\n",
    "\n",
    "# #sort by clear percentage\n",
    "# try:\n",
    "#     # df_tiles_sorted = gdf_tiles_contain_aoi.sort_values(by='clear_percent', ascending=False)\n",
    "#     df_tiles_sorted = gdf_tiles_contain_aoi.sort_values(by='clear_confidence_percent', ascending=False)\n",
    "#     # Drop duplicates and keep the first occurrence of each date (highest clear percentage)\n",
    "#     df_tiles_unique = df_tiles_sorted.drop_duplicates(subset='date_acquired', keep='first')\n",
    "# except:\n",
    "#     print(\"Couldn't sort by clear confidence percentage\")\n",
    "#     df_tiles_unique = gdf_tiles_contain_aoi.drop_duplicates(subset='date_acquired', keep='first')\n",
    "\n",
    "gdf_tiles.to_csv(\"C:\\\\Users\\\\msmurphy\\\\Documents\\\\WinterTurf Planet\\\\Sensor Courses\\\\Output_Index_All_Test.csv\")\n",
    "gdf_tiles_contain_aoi.to_csv(\"C:\\\\Users\\\\msmurphy\\\\Documents\\\\WinterTurf Planet\\\\Sensor Courses\\\\Output_Index_Contain_AOI_Test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
