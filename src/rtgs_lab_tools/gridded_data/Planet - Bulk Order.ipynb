{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-15 21:34:15.673118\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "import geojson\n",
    "import json\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon, shape\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "\n",
    "\n",
    "PLANET_API_KEY=''\n",
    "\n",
    "orders_url = 'https://api.planet.com/compute/ops/orders/v2'\n",
    "\n",
    "# set up requests to work with api\n",
    "auth = HTTPBasicAuth(PLANET_API_KEY, '')\n",
    "headers = {'content-type': 'application/json'}\n",
    "\n",
    "# Define the path to your geodatabase and feature class\n",
    "gdb_path = 'C:\\\\Users\\\\msmurphy\\\\Documents\\\\WinterTurf\\\\projects\\\\DEM WinterTurf\\\\DEM WinterTurf\\\\WinterTurf_Greens_21_24.gdb'\n",
    "input_excel_courses = \"C:\\\\Users\\\\msmurphy\\\\Documents\\\\WinterTurf Planet\\\\Sensor Courses\\\\Wausau 2025 input.xlsx\"\n",
    "\n",
    "#Directory paths\n",
    "bDir = 'C:\\\\Users\\\\msmurphy\\\\Documents\\\\WinterTurf Planet\\\\Sensor Courses\\\\'\n",
    "iDir = bDir + 'json'\n",
    "oDir = bDir + 'images'\n",
    "\n",
    "\n",
    "#output variables\n",
    "order_nickname = \"Wausau 2021 PSBSD\"\n",
    "output_file_path = \"C:\\\\Users\\\\msmurphy\\\\Documents\\\\WinterTurf Planet\\\\Sensor Courses\\\\Wausau 2021 PSBSD.csv\"\n",
    "\n",
    "print(datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-15 21:24:19.994803\n"
     ]
    }
   ],
   "source": [
    "#some json functions\n",
    "\n",
    "def open_geojson(file_path):\n",
    "    with open(file_path) as f:\n",
    "#         gj = geojson.load(f)\n",
    "        gj = json.load(f)\n",
    "        \n",
    "    return gj\n",
    "\n",
    "def get_geojson_geometry(geojson):\n",
    "    geometry = [i['geometry'] for i in geojson['features']]\n",
    "    \n",
    "    return geometry\n",
    "\n",
    "def plot_wrapper(gdf, ax, color, linewidth=1.5):\n",
    "    '''Convenience function for overlaying spatial data on the same plot'''\n",
    "    gdf.plot(\n",
    "        facecolor=\"none\",\n",
    "        edgecolor=color,\n",
    "        linewidth=linewidth,\n",
    "        ax = ax   \n",
    "    )\n",
    "\n",
    "# define helpful functions for submitting, polling, and downloading an order\n",
    "def place_order(request, auth):\n",
    "    response = requests.post(orders_url, data=json.dumps(request), auth=auth, headers=headers)\n",
    "    print(response)\n",
    "    \n",
    "    if not response.ok:\n",
    "        raise Exception(response.content)\n",
    "\n",
    "    order_id = response.json()['id']\n",
    "    print(order_id)\n",
    "    order_url = orders_url + '/' + order_id\n",
    "    return order_url\n",
    "\n",
    "# print(datetime.now())\n",
    "\n",
    "# Function to retrieve all features\n",
    "def get_all_features(search_params, auth):\n",
    "    features = []\n",
    "    offset = 0\n",
    "    while True:\n",
    "        search_params['offset'] = offset\n",
    "        response = requests.get(orders_url, headers=headers, auth=auth, params=search_params)\n",
    "        response.raise_for_status()  # Raise an error for bad requests\n",
    "        data = response.json()\n",
    "        # print(data)\n",
    "        features.extend(data['features'])\n",
    "        if len(data['features']) < 250:\n",
    "            break\n",
    "        offset += 250\n",
    "    return features\n",
    "\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course_02I_Outline 2021  total number of images: 174\n",
      "*************** Tiles we want ***************\n",
      "['20210614_160521_77_245a', '20210815_164912_42_2408', '20210705_160303_71_2212', '20210422_160149_05_2444', '20210901_161231_97_225a', '20210618_161110_47_2280', '20210616_164930_56_240a', '20210320_165506_51_2406', '20210610_165346_86_2406', '20210605_161226_56_2441', '20210604_160251_85_2448', '20210529_160352_40_2430', '20210430_161444_51_2233', '20210823_160028_67_2450', '20210702_160155_38_2447', '20210913_155839_92_2428', '20210928_165012_47_2405', '20210814_160335_07_242a', '20210711_165426_75_240c', '20210929_160306_13_2440', '20210923_160138_18_2235', '20210919_160101_32_245c', '20210918_165250_42_227c', '20210816_160107_80_2420', '20210612_155946_11_2460', '20210701_160332_50_2432', '20210531_161216_02_2280', '20210518_160549_49_2436', '20210724_160414_51_2436', '20210813_155925_42_242b', '20210407_160119_57_2458', '20210906_160539_86_220b', '20210417_165619_65_225b', '20210319_160739_20_245c', '20210313_165550_81_2406', '20210915_165207_81_2405', '20210329_160250_27_2432', '20211014_160032_55_2447', '20211017_165229_85_2405', '20211026_155942_71_2432', '20211030_164743_34_2405', '20210802_160038_22_2442', '20210402_160218_14_2451', '20210403_160154_97_2420', '20210330_160844_86_2262', '20210401_160535_14_245e', '20210405_160608_51_241e', '20211104_164917_11_240a', '20210721_160209_29_2465', '20210416_161031_22_2441', '20210428_160541_90_245e', '20210715_165020_01_2274', '20210819_155952_67_2428', '20210909_164918_54_227c', '20210601_165121_26_2426', '20210318_165733_36_2401', '20210606_160329_78_242b', '20210608_160233_07_2442', '20211106_160214_80_241f', '20210630_160427_62_2463', '20211108_155912_42_2464', '20211123_164839_64_2254', '20210712_165125_94_240a', '20211025_160339_43_2421', '20211128_160135_49_242b', '20210930_164710_22_2307', '20210727_160037_87_2451', '20210406_160702_95_2421', '20210710_160311_83_242b', '20210314_160606_53_2434', '20211105_155947_20_2212', '20210125_165532_28_2403', '20211130_165155_57_2405', '20210122_165742_62_2413', '20210718_165143_35_2413', '20210804_161346_50_2251', '20210303_160304_09_2458', '20210309_161459_71_2276', '20211225_161335_65_2262', '20211204_164855_47_227a', '20210719_155947_00_2435', '20210308_165357_90_2307', '20210306_165620_69_2406', '20210304_160230_06_2212', '20210217_160942_85_2251', '20210226_165531_72_241c', '20210123_165430_37_2406', '20210301_161020_10_2264', '20210305_165457_59_2413', '20211229_160240_05_2465', '20210225_165715_36_2424', '20211208_164810_53_2407', '20210110_165453_32_240c', '20210129_165746_85_2413', '20211115_160016_58_245c', '20210212_165715_98_2413']\n",
      "\n",
      "*************** Tiles we already have ***************\n",
      "['20210727_160037_87_2451', '20210319_160739_20_245c', '20210330_160844_86_2262', '20210715_165020_01_2274', '20210110_165453_32_240c', '20210630_160427_62_2463', '20210724_160414_51_2436', '20210318_165733_36_2401', '20211108_155912_42_2464', '20210416_161031_22_2441', '20210422_160149_05_2444', '20210610_165346_86_2406', '20210304_160230_06_2212', '20210702_160155_38_2447', '20211130_165155_57_2405', '20210919_160101_32_245c', '20210823_160028_67_2450', '20211208_164810_53_2407', '20210712_165125_94_240a', '20210606_160329_78_242b', '20210701_160332_50_2432', '20210930_164710_22_2307', '20210301_161020_10_2264', '20210705_160303_71_2212', '20211017_165229_85_2405', '20210403_160154_97_2420', '20210710_160311_83_242b', '20210407_160119_57_2458', '20210604_160251_85_2448', '20210618_161110_47_2280', '20210913_155839_92_2428', '20211123_164839_64_2254', '20210402_160218_14_2451', '20210401_160535_14_245e', '20210529_160352_40_2430', '20210309_161459_71_2276', '20210928_165012_47_2405', '20210518_160549_49_2436', '20210329_160250_27_2432', '20211030_164743_34_2405', '20210923_160138_18_2235', '20210430_161444_51_2233', '20211128_160135_49_242b', '20210909_164918_54_227c', '20210819_155952_67_2428', '20210123_165430_37_2406', '20210308_165357_90_2307', '20210721_160209_29_2465', '20210417_165619_65_225b', '20210802_160038_22_2442', '20210804_161346_50_2251', '20210217_160942_85_2251', '20210915_165207_81_2405', '20211204_164855_47_227a', '20210531_161216_02_2280', '20210608_160233_07_2442', '20211105_155947_20_2212', '20210719_155947_00_2435', '20210306_165620_69_2406', '20210605_161226_56_2441', '20210305_165457_59_2413', '20211104_164917_11_240a']\n",
      "\n",
      "*************** Order list ***************\n",
      "['20210614_160521_77_245a', '20210815_164912_42_2408', '20210901_161231_97_225a', '20210616_164930_56_240a', '20210320_165506_51_2406', '20210814_160335_07_242a', '20210711_165426_75_240c', '20210929_160306_13_2440', '20210918_165250_42_227c', '20210816_160107_80_2420', '20210612_155946_11_2460', '20210813_155925_42_242b', '20210906_160539_86_220b', '20210313_165550_81_2406', '20211014_160032_55_2447', '20211026_155942_71_2432', '20210405_160608_51_241e', '20210428_160541_90_245e', '20210601_165121_26_2426', '20211106_160214_80_241f', '20211025_160339_43_2421', '20210406_160702_95_2421', '20210314_160606_53_2434', '20210125_165532_28_2403', '20210122_165742_62_2413', '20210718_165143_35_2413', '20210303_160304_09_2458', '20211225_161335_65_2262', '20210226_165531_72_241c', '20211229_160240_05_2465', '20210225_165715_36_2424', '20210129_165746_85_2413', '20211115_160016_58_245c', '20210212_165715_98_2413']\n",
      "Course_02I_Outline: ordering 34 tiles.\n",
      "<Response [202]>\n",
      "698db218-a798-4643-8f0c-e212d6bb5877\n",
      "finished Course_02I_Outline 2025-08-15 21:36:46.214723\n"
     ]
    }
   ],
   "source": [
    "file_path = input_excel_courses\n",
    "df_courses = pd.read_excel(file_path)\n",
    "\n",
    "#create a dataframe for the output\n",
    "\n",
    "# df_output = pd.DataFrame(columns=['Course','Order_2017','Order_2018','Order_2019','Order_2020','Order_2021','Order_2022','Order_2023', 'Order_2024', 'Order_2025'])\n",
    "df_output = pd.DataFrame(columns=['Course','Order_2021'])\n",
    "df_output['Course'] = df_courses['Course']\n",
    "\n",
    "for index, row in df_courses.iterrows(): \n",
    "    course = row['Course']\n",
    "    feature_class_name = course\n",
    "    # years = ['2025']\n",
    "    years = ['2021']\n",
    "    for year in years:\n",
    "        start_date = year+'-01-01'\n",
    "        end_date = year+'-12-31'\n",
    "\n",
    "        if not os.path.exists(iDir+\"\\\\\"+feature_class_name+\".geojson\"):\n",
    "            arcpy.conversion.FeaturesToJSON(\n",
    "                in_features=gdb_path+\"\\\\\"+feature_class_name,\n",
    "                out_json_file=iDir+\"\\\\\"+feature_class_name+\".geojson\",\n",
    "                format_json=\"NOT_FORMATTED\",\n",
    "                include_z_values=\"NO_Z_VALUES\",\n",
    "                include_m_values=\"NO_M_VALUES\",\n",
    "                geoJSON=\"GEOJSON\",\n",
    "                outputToWGS84=\"WGS84\",\n",
    "                use_field_alias=\"USE_FIELD_NAME\"\n",
    "            )\n",
    "\n",
    "        geojson_file = iDir+\"\\\\\"+feature_class_name+\".geojson\"\n",
    "\n",
    "        # Read the GeoJSON file into a GeoDataFrame for the aoi\n",
    "        gdf_aoi = gpd.read_file(geojson_file)\n",
    "\n",
    "        #Create the geometry for the Planet query\n",
    "        boundary_geojson = open_geojson(geojson_file)\n",
    "\n",
    "        bounds = get_geojson_geometry(boundary_geojson)\n",
    "\n",
    "        geojson_geometry = bounds[0]\n",
    "\n",
    "        #Set up filters\n",
    "        geometry_filter = {\n",
    "        \"type\": \"GeometryFilter\",\n",
    "        \"field_name\": \"geometry\",\n",
    "        \"config\": geojson_geometry\n",
    "        }\n",
    "\n",
    "        instrument_filter = {\n",
    "        \"type\":\"StringInFilter\",\n",
    "        \"field_name\":\"instrument\",\n",
    "        \"config\":[\n",
    "            \"PSB.SD\"\n",
    "            # \"PS2.SD\"\n",
    "            # \"PS2\"\n",
    "        ]\n",
    "        }\n",
    "\n",
    "        quality_category_filter = {\n",
    "        \"type\":\"StringInFilter\",\n",
    "        \"field_name\":\"quality_category\",\n",
    "        \"config\":[\n",
    "            \"standard\"\n",
    "        ]\n",
    "        }\n",
    "\n",
    "        date_range_filter = {\n",
    "        \"type\": \"DateRangeFilter\",\n",
    "        \"field_name\": \"acquired\",\n",
    "        \"config\": {\n",
    "            \"gte\": start_date+\"T00:00:00.000Z\",\n",
    "            \"lte\": end_date+\"T00:00:00.000Z\"\n",
    "        }\n",
    "        }\n",
    "\n",
    "        # only get images which have <5% cloud coverage\n",
    "        cloud_cover_filter = {\n",
    "        \"type\": \"RangeFilter\",\n",
    "        \"field_name\": \"cloud_cover\",\n",
    "        \"config\": {\n",
    "            \"lte\": 0.05\n",
    "        }\n",
    "        }\n",
    "\n",
    "        # orthoanalytic images that have 4 bands and are corrected for surface reflectance\n",
    "        asset_filter = {\n",
    "        \"type\": \"AssetFilter\",\n",
    "        \"config\": [\"ortho_analytic_4b_sr\"]\n",
    "        }\n",
    "\n",
    "        # combine filters including clear filters\n",
    "        was_combined_filter = {\n",
    "        \"type\": \"AndFilter\",\n",
    "        \"config\": [geometry_filter, date_range_filter, quality_category_filter, cloud_cover_filter, asset_filter, instrument_filter]\n",
    "        }\n",
    "\n",
    "        item_type = \"PSScene\"\n",
    "\n",
    "        # API request object\n",
    "        search_request = {\n",
    "        \"item_types\": [item_type], \n",
    "        # \"limit\":250,\n",
    "        # \"offset\": 0,\n",
    "        \"filter\": was_combined_filter\n",
    "        }\n",
    "\n",
    "        # # fire off the POST request\n",
    "        search_result = \\\n",
    "        requests.post(\n",
    "            'https://api.planet.com/data/v1/quick-search?_sort=acquired asc&_page_size=250',\n",
    "            auth=HTTPBasicAuth(PLANET_API_KEY, ''),\n",
    "            json=search_request)\n",
    "\n",
    "        # print(search_result.json()['_links'])\n",
    "        try:\n",
    "            next_url = search_result.json()['_links']['_next']\n",
    "        except:\n",
    "            next_url = None\n",
    "\n",
    "        next_ids = []\n",
    "        image_ids = []\n",
    "\n",
    "        if next_url is not None:\n",
    "            next = requests.get(next_url, auth=auth)\n",
    "            next_ids = [feature['id'] for feature in next.json()['features']]\n",
    "            output_features = search_result.json()['features'] + next.json()['features']\n",
    "        else:\n",
    "            output_features = search_result.json()['features']\n",
    "        \n",
    "\n",
    "        image_ids = [feature['id'] for feature in search_result.json()['features']]\n",
    "        \n",
    "        image_ids = image_ids + next_ids        \n",
    "\n",
    "        image_ids.sort()\n",
    "        print(course, year, ' total number of images:', len(image_ids))\n",
    "\n",
    "        #Create a dataframe out of the json response to the search\n",
    "        # df_tiles = pd.json_normalize(search_result.json()['features'], max_level=1)\n",
    "        df_tiles = pd.json_normalize(output_features, max_level=1)\n",
    "        # print(len(df_tiles))\n",
    "\n",
    "        # rename properties column, dropping the properties label\n",
    "        prop_cols = {col: col.split('.')[1] for col in df_tiles.columns if 'properties' in col}\n",
    "        df_tiles.rename(columns=prop_cols, inplace=True)\n",
    "\n",
    "        geoms = [Polygon(geo[0]) for geo in df_tiles['geometry.coordinates']]\n",
    "        gdf_tiles = gpd.GeoDataFrame(df_tiles, geometry = geoms, crs=4326)\n",
    "\n",
    "        gdf_tiles_contain_aoi = gpd.sjoin(gdf_tiles, gdf_aoi,  how='inner', predicate='contains' )\n",
    "\n",
    "        gdf_tiles_contain_aoi['date_acquired'] = pd.to_datetime(gdf_tiles_contain_aoi['acquired'].str[:10])\n",
    "\n",
    "        #sort by clear percentage\n",
    "        try:\n",
    "            # df_tiles_sorted = gdf_tiles_contain_aoi.sort_values(by='clear_percent', ascending=False)\n",
    "            df_tiles_sorted = gdf_tiles_contain_aoi.sort_values(by='clear_confidence_percent', ascending=False)\n",
    "            # Drop duplicates and keep the first occurrence of each date (highest clear percentage)\n",
    "            df_tiles_unique = df_tiles_sorted.drop_duplicates(subset='date_acquired', keep='first')\n",
    "        except:\n",
    "            print(\"Couldn't sort by clear confidence percentage\")\n",
    "            df_tiles_unique = gdf_tiles_contain_aoi.drop_duplicates(subset='date_acquired', keep='first')\n",
    "\n",
    "\n",
    "        df_tiles_unique.to_csv(\"C:\\\\Users\\\\msmurphy\\\\Documents\\\\WinterTurf Planet\\\\Sensor Courses\\\\\"+course+\"_\"+year+\"PSBSD_downloaded.csv\")\n",
    "\n",
    "\n",
    "        ############### Check if we already have any of the tiles we want to order ###################\n",
    "\n",
    "        # Set the workspace to your geodatabase\n",
    "        arcpy.env.workspace = \"C:\\\\Users\\\\msmurphy\\\\Documents\\\\ArcGIS\\\\Projects\\\\Greens\\\\\"+course+\".gdb\"\n",
    "\n",
    "        # List all rasters in the geodatabase\n",
    "        rasters = arcpy.ListRasters()\n",
    "\n",
    "        # Filter rasters that end with \"3B_AnalyticMS_SR_clip\"\n",
    "        filtered_rasters = [raster for raster in rasters if raster.endswith(\"3B_AnalyticMS_SR_clip\")]\n",
    "\n",
    "        # Extract the beginning of the raster names\n",
    "        acquired_tiles = [raster[1:-22] for raster in filtered_rasters]\n",
    "\n",
    "        filtered_list = df_tiles_unique['id'].to_list()\n",
    "\n",
    "        # Print the list\n",
    "        print(\"*************** Tiles we want ***************\")\n",
    "        print(filtered_list)\n",
    "        print()\n",
    "\n",
    "        common_values = list(set(acquired_tiles) & set(filtered_list)) \n",
    "        print(\"*************** Tiles we already have ***************\")\n",
    "        print(common_values)\n",
    "        print()\n",
    "\n",
    "        order_list = [item for item in filtered_list if item not in acquired_tiles] \n",
    "        print(\"*************** Order list ***************\")\n",
    "        print(order_list)\n",
    "\n",
    "        #if you are not going to filter by tiles we already have\n",
    "        #order_list = df_tiles_unique['id'].to_list()\n",
    "\n",
    "        num_tiles = len(order_list)\n",
    "\n",
    "        print(course+\": ordering \"+str(len(order_list))+\" tiles.\")\n",
    "\n",
    "        course_product = [\n",
    "        {\n",
    "        \"item_ids\": order_list,\n",
    "        \"item_type\": \"PSScene\",\n",
    "        \"product_bundle\": \"analytic_sr_udm2\"\n",
    "        }\n",
    "        ]\n",
    "\n",
    "        # define the clip tool\n",
    "        clip = {\n",
    "            \"clip\": {\n",
    "                \"aoi\": geojson_geometry\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # create an order request with the clipping tool\n",
    "        request_clip = {\n",
    "        \"name\": order_nickname+\" \"+course+\" \"+year,\n",
    "        \"products\": course_product,\n",
    "        \"tools\": [clip]\n",
    "        }\n",
    "\n",
    "\n",
    "        clip_order_url = place_order(request_clip, auth)\n",
    "\n",
    "        df_output.loc[df_output['Course'] == course, 'Order_'+year] = clip_order_url\n",
    "\n",
    "        print(\"finished \"+course+\" \"+str(datetime.now()))\n",
    "\n",
    "df_output.to_csv(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the above fails, output the partial df of order numbers\n",
    "df_output.to_csv(output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
