{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3938bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterstats import zonal_stats\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import regex\n",
    "\n",
    "\n",
    "geotiff_path = 'C:\\\\Users\\\\msmurphy\\\\Documents\\\\WinterTurf Planet\\\\Sensor Courses\\\\images\\\\All_Sensor_Courses\\\\Course_01A_Outline\\\\2698245e-873a-4aeb-9d76-97353a81172b\\PSScene\\\\20240604_162151_41_24d0_3B_AnalyticMS_SR_clip.tif'\n",
    "\n",
    "metadata_json_path = 'C:\\\\Users\\\\msmurphy\\\\Documents\\\\WinterTurf Planet\\\\Sensor Courses\\\\images\\\\All_Sensor_Courses\\\\Course_01A_Outline\\\\2698245e-873a-4aeb-9d76-97353a81172b\\PSScene\\\\20240604_162151_41_24d0_metadata.json'\n",
    "\n",
    "ndvi_path = 'C:\\\\Users\\\\msmurphy\\\\Documents\\\\WinterTurf Planet\\\\Sensor Courses\\\\images\\\\All_Sensor_Courses\\\\Course_01A_Outline\\\\2698245e-873a-4aeb-9d76-97353a81172b\\PSScene\\\\20240604_162151_41_24d0_3B_AnalyticMS_SR_clip_ndvi.tif'\n",
    "\n",
    "greens_path = \"C:\\\\Users\\\\msmurphy\\\\Documents\\\\WinterTurf Planet\\\\Sensor Courses\\\\greens-geojson\\\\Course_01A_Greens.geojson\"\n",
    "\n",
    "greens_projected_path = \"C:\\\\Users\\\\msmurphy\\\\Documents\\\\WinterTurf Planet\\\\Sensor Courses\\\\greens-geojson\\\\Course_01A_Greens_Projected.geojson\"\n",
    "\n",
    "output_csv_path = \"C:\\\\Users\\\\msmurphy\\\\Documents\\\\WinterTurf Planet\\\\Sensor Courses\\\\output-csv\\\\Course_01A.csv\"\n",
    "\n",
    "output_xlsx_path = \"C:\\\\Users\\\\msmurphy\\\\Documents\\\\WinterTurf Planet\\\\Sensor Courses\\\\output-csv\\\\Course_01A.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "655e1b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target CRS from TIFF: EPSG:32615\n",
      "Saved reprojected GeoJSON: C:\\Users\\msmurphy\\Documents\\WinterTurf Planet\\Sensor Courses\\greens-geojson\\Course_01A_Greens_Projected.geojson\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Read the target CRS from the GeoTIFF\n",
    "with rasterio.open(geotiff_path) as src:\n",
    "    tif_crs = src.crs\n",
    "\n",
    "if tif_crs is None:\n",
    "    raise ValueError(\"The GeoTIFF does not have a defined CRS. Please set it before proceeding.\")\n",
    "\n",
    "print(\"Target CRS from TIFF:\", tif_crs)\n",
    "\n",
    "# 2) Load the GeoJSON\n",
    "gdf = gpd.read_file(greens_path)\n",
    "\n",
    "\n",
    "# 4) Reproject to the TIFF's CRS\n",
    "gdf_matched = gdf.to_crs(tif_crs)\n",
    "\n",
    "# 5) Save as GeoJSON\n",
    "gdf_matched.to_file(greens_projected_path, driver=\"GeoJSON\")\n",
    "print(f\"Saved reprojected GeoJSON: {greens_projected_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b4c2915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDVI calculated and saved to C:\\Users\\msmurphy\\Documents\\WinterTurf Planet\\Sensor Courses\\images\\All_Sensor_Courses\\Course_01A_Outline\\2698245e-873a-4aeb-9d76-97353a81172b\\PSScene\\20240604_162151_41_24d0_3B_AnalyticMS_SR_clip_ndvi.tif\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with rasterio.open(geotiff_path) as src:\n",
    "    # Read the Red (Band 3) and NIR (Band 4) bands\n",
    "    # Rasterio uses 1-based indexing for bands\n",
    "    red = src.read(3).astype(float)\n",
    "    nir = src.read(4).astype(float)\n",
    "    \n",
    "    # Get the metadata from the source file to use for the output file\n",
    "    meta = src.meta\n",
    "\n",
    "# Calculate NDVI\n",
    "# Use numpy.seterr to ignore division by zero warnings, which are handled by np.where\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "ndvi = np.where(\n",
    "    (nir + red) == 0., # Condition to check for zero division\n",
    "    0,                # Value to use if condition is true (handles no-data areas)\n",
    "    (nir - red) / (nir + red) # The NDVI formula\n",
    ")\n",
    "\n",
    "# Update the metadata for the new NDVI file\n",
    "meta.update(\n",
    "    dtype=rasterio.float32, # NDVI values are floats between -1 and 1\n",
    "    count=1,                # The output will have only one band\n",
    "    compress='lzw'          # Optional: adds compression\n",
    ")\n",
    "\n",
    "# Write the NDVI array to a new GeoTIFF file\n",
    "with rasterio.open(ndvi_path, 'w', **meta) as dst:\n",
    "    dst.write(ndvi.astype(rasterio.float32), 1)\n",
    "\n",
    "print(f\"NDVI calculated and saved to {ndvi_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0403218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some helper functions for filtering the green numbers and flattening the dictionary\n",
    "\n",
    "def flatten_dict(d, parent_key=\"\", sep=\".\"):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "\n",
    "def parse_green_no(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    if isinstance(val, (int, float)):\n",
    "        n = int(val)\n",
    "        return n if 1 <= n <= 18 else None\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ebad394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load & flatten metadata JSON\n",
    "\n",
    "with open(metadata_json_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "if isinstance(metadata, list):\n",
    "    # If the JSON is a list, take the first item or merge as needed\n",
    "    # Here we take the first item for a single-row summary\n",
    "    if not metadata:\n",
    "        raise ValueError(\"metadata.json is an empty list.\")\n",
    "    metadata = metadata[0]\n",
    "\n",
    "if not isinstance(metadata, dict):\n",
    "    raise ValueError(\"metadata.json must be an object (or a list of objects).\")\n",
    "\n",
    "meta_flat = flatten_dict(metadata)\n",
    "meta_df = pd.DataFrame([meta_flat])  # single-row DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fe0734e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NDVI summary row ===\n",
      "                     id    type                    acquired  anomalous_pixels  clear_confidence_percent  clear_percent  cloud_cover  cloud_percent  ground_control  gsd  heavy_haze_percent instrument item_type  light_haze_percent  pixel_resolution    provider            published publishing_stage quality_category  satellite_azimuth satellite_id  shadow_percent  snow_ice_percent strip_id  sun_azimuth  sun_elevation              updated  view_angle  visible_confidence_percent  visible_percent acquired_date acquired_time  green_1  green_2  green_3  green_4  green_5  green_6  green_7  green_8  green_9  green_10  green_11  green_12  green_13  green_14  green_15  green_16  green_17  green_18  overall_ndvi\n",
      "20240604_162151_41_24d0 Feature 2024-06-04T16:21:51.419405Z                 0                        98            100          0.0              0            True  3.7                   0     PSB.SD   PSScene                   0                 3 planetscope 2024-06-04T22:11:26Z        finalized         standard              103.2         24d0               0                 0  7350941        126.6           58.7 2024-06-05T08:51:38Z         3.4                          98              100    2024-06-04      11:21:51 0.828301 0.835011 0.832735 0.842862 0.842833 0.838065 0.855223  0.84953 0.778636  0.816938  0.843229  0.846198  0.826506  0.814136   0.82383  0.850739   0.83474  0.838393      0.833416\n",
      "\n",
      "Saved CSV: C:\\Users\\msmurphy\\Documents\\WinterTurf Planet\\Sensor Courses\\output-csv\\Course_01A.csv\n",
      "Saved Excel: C:\\Users\\msmurphy\\Documents\\WinterTurf Planet\\Sensor Courses\\output-csv\\Course_01A.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --- Read raster metadata ---\n",
    "with rasterio.open(ndvi_path) as src:\n",
    "    raster_nodata = src.nodata\n",
    "# print(\"Raster NoData:\", raster_nodata)\n",
    "\n",
    "# --- Load projected GeoJSON ---\n",
    "gdf = gpd.read_file(greens_projected_path)\n",
    "\n",
    "# Ensure the 'green_no' field exists\n",
    "if \"green_no\" not in gdf.columns:\n",
    "    raise KeyError(\"Field 'green_no' not found in GeoJSON properties.\")\n",
    "\n",
    "# --- Prepare GeoJSON-like shapes with properties ---\n",
    "shapes = json.loads(gdf.to_json())\n",
    "\n",
    "# --- Zonal stats: get pixel SUM and COUNT per feature ---\n",
    "stats = zonal_stats(\n",
    "    shapes,                # FeatureCollection with geometry + properties\n",
    "    ndvi_path,\n",
    "    stats=[\"sum\", \"count\"],  # sum of pixel values, and number of valid pixels\n",
    "    nodata=raster_nodata,    # exclude nodata pixels\n",
    "    geojson_out=True,\n",
    "    all_touched=False        # set True for more inclusive boundary behavior if desired\n",
    ")\n",
    "\n",
    "# --- Build a DataFrame of per-feature results ---\n",
    "rows = []\n",
    "\n",
    "for feat in stats:\n",
    "    props = feat.get(\"properties\", {}) or {}\n",
    "    gn = parse_green_no(props.get(\"green_no\"))\n",
    "\n",
    "    sum_val = props.get(\"sum\")\n",
    "    count_val = props.get(\"count\")\n",
    "\n",
    "    # Skip features with no valid pixels\n",
    "    if gn is None or sum_val is None or count_val in (None, 0):\n",
    "        continue\n",
    "\n",
    "    rows.append({\"green_no\": gn, \"sum\": float(sum_val), \"count\": int(count_val)})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "if df.empty:\n",
    "    raise ValueError(\"No valid NDVI pixels found for any feature with a valid green_no in [1..18].\")\n",
    "\n",
    "\n",
    "grouped = df.groupby(\"green_no\", as_index=False).agg(\n",
    "    total_sum=(\"sum\", \"sum\"),\n",
    "    total_count=(\"count\", \"sum\"),\n",
    ")\n",
    "grouped[\"mean_ndvi\"] = grouped[\"total_sum\"] / grouped[\"total_count\"]\n",
    "\n",
    "# -------------------\n",
    "# Build columns green_1..green_18 and overall_ndvi\n",
    "# -------------------\n",
    "green_cols = {}\n",
    "for n in range(1, 19):\n",
    "    val = grouped.loc[grouped[\"green_no\"] == n, \"mean_ndvi\"]\n",
    "    green_cols[f\"green_{n}\"] = (val.iloc[0] if len(val) == 1 else pd.NA)\n",
    "\n",
    "# Overall pixel-weighted mean across all pixels in any feature\n",
    "overall_sum = grouped[\"total_sum\"].sum()\n",
    "overall_count = grouped[\"total_count\"].sum()\n",
    "overall_ndvi = overall_sum / overall_count\n",
    "\n",
    "# -------------------\n",
    "# Combine metadata + NDVI summary into a single-row DataFrame\n",
    "# -------------------\n",
    "out_row = {**meta_flat, **green_cols, \"overall_ndvi\": overall_ndvi}\n",
    "out_df = pd.DataFrame([out_row])\n",
    "\n",
    "# Optional: reorder columns (metadata first, then green_1..green_18, then overall_ndvi)\n",
    "meta_cols = list(meta_flat.keys())\n",
    "ndvi_cols = [f\"green_{n}\" for n in range(1, 19)] + [\"overall_ndvi\"]\n",
    "ordered_cols = meta_cols + ndvi_cols\n",
    "out_df = out_df.reindex(columns=ordered_cols)\n",
    "\n",
    "#configure columns to remove properties header and all geometry info\n",
    "cols_to_drop = [c for c in out_df.columns if c.startswith(\"geometry.\")]\n",
    "out_df = out_df.drop(columns=cols_to_drop)\n",
    "\n",
    "out_df.columns = out_df.columns.str.replace(r'^properties\\.', '', regex=True)\n",
    "\n",
    "#put the date time into local\n",
    "# 1) Parse to timezone-aware UTC datetime\n",
    "out_df[\"acquired_dt\"] = pd.to_datetime(out_df[\"acquired\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "# 2) Convert to local US Central time (handles CST/CDT automatically)\n",
    "out_df[\"acquired_dt_local\"] = out_df[\"acquired_dt\"].dt.tz_convert(\"America/Chicago\")\n",
    "\n",
    "# 3) Create date and time columns (local time)\n",
    "out_df[\"acquired_date\"] = out_df[\"acquired_dt_local\"].dt.date\n",
    "out_df[\"acquired_time\"] = out_df[\"acquired_dt_local\"].dt.strftime(\"%H:%M:%S\")\n",
    "\n",
    "# 4)  Drop helper columns\n",
    "out_df = out_df.drop(columns=[\"acquired_dt\", \"acquired_dt_local\"])\n",
    "\n",
    "\n",
    "# Identify green and special columns\n",
    "green_cols_all = [f\"green_{n}\" for n in range(1, 19)]\n",
    "special_cols = set(green_cols_all + [\"overall_ndvi\", \"acquired_date\", \"acquired_time\"])\n",
    "\n",
    "# Metadata columns = everything else that's not special\n",
    "meta_cols = [c for c in out_df.columns if c not in special_cols and not c.startswith(\"green_\")]\n",
    "\n",
    "# Keep only green columns that actually exist (in case some greens are missing)\n",
    "green_cols_existing = [c for c in green_cols_all if c in out_df.columns]\n",
    "\n",
    "# Build the desired order:\n",
    "ordered_cols = meta_cols + [\"acquired_date\", \"acquired_time\"] + green_cols_existing + [\"overall_ndvi\"]\n",
    "\n",
    "# Finally, reindex to that order (only keep columns that exist)\n",
    "ordered_cols = [c for c in ordered_cols if c in out_df.columns]\n",
    "out_df = out_df.reindex(columns=ordered_cols)\n",
    "\n",
    "\n",
    "# Save outputs\n",
    "out_df.to_csv(output_csv_path, index=False)\n",
    "try:\n",
    "    out_df.to_excel(output_xlsx_path, index=False, engine=\"openpyxl\")\n",
    "except Exception as e:\n",
    "    print(\"Excel export failed:\", e)\n",
    "\n",
    "print(\"\\n=== NDVI summary row ===\")\n",
    "print(out_df.to_string(index=False))\n",
    "print(f\"\\nSaved CSV: {output_csv_path}\")\n",
    "print(f\"Saved Excel: {output_xlsx_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
